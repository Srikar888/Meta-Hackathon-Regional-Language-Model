# -*- coding: utf-8 -*-
"""Regional Language Implementation llama3_2_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gvQ8ZBujU_E8pkljzGdr15aL04V7lD-d

### Install Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index==0.10.18 llama-index-llms-groq==0.1.3 groq==0.4.2 llama-index-embeddings-huggingface==0.2.0

"""### Import Libraries"""

from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    ServiceContext,
    load_index_from_storage
)
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.groq import Groq
# import os
# from dotenv import load_dotenv
# load_dotenv()
import warnings
warnings.filterwarnings('ignore')

GROQ_API_KEY = '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'

# GROQ_API_KEY = os.getenv("GROQ_API_KEY")

"""### Data Ingestion"""

# data ingestion
reader = SimpleDirectoryReader(input_files=["/content/rtiact-kannada.pdf"])
documents = reader.load_data()

"""https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/"""

len(documents)

documents[0].metadata
documents[1].metadata

"""### Chunking"""

text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=200)
nodes = text_splitter.get_nodes_from_documents(documents, show_progress=True)

"""https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"""

len(nodes)

nodes[0].metadata

"""https://chunkviz.up.railway.app/

### Embedding Model
"""

embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")

"""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2

https://huggingface.co/spaces/mteb/leaderboard

### Define LLM Model
"""

llm = Groq(model="llama-3.2-1b-preview", api_key=GROQ_API_KEY)

"""https://console.groq.com/docs/models"""



"""https://console.groq.com/keys

### Configure Service Context
"""

service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)

"""### Create Vector Store Index"""

vector_index = VectorStoreIndex.from_documents(documents, show_progress=True, service_context=service_context, node_parser=nodes)

"""https://docs.llamaindex.ai/en/stable/module_guides/indexing/vector_store_index/

#### Persist/Save Index
"""

vector_index.storage_context.persist(persist_dir="./storage_mini")

"""#### Define Storage Context"""

storage_context = StorageContext.from_defaults(persist_dir="./storage_mini")

"""https://docs.llamaindex.ai/en/stable/api_reference/storage/storage_context/

#### Load Index
"""

index = load_index_from_storage(storage_context, service_context=service_context)

"""### Define Query Engine"""

query_engine = index.as_query_engine(service_context=service_context)

"""https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/

#### Feed in user query

https://docs.llamaindex.ai/en/stable/examples/prompts/prompts_rag/#viewingcustomizing-prompts
"""

query = "ಬ್ರಹ್ಮ॑ಣಾಂ ಬ್ರಹ್ಮಣೋ ಬ್ರಹ್ಮಣೋ॒ ಬ್ರಹ್ಮ॑ಣಾಂ॒ ಬ್ರಹ್ಮ॑ಣಾಂ ಬ್ರಹ್ಮಣಸ್ಪತೇ ಪತೇಬ್ರಹ್ಮ"
resp = query_engine.query(query)

def check_verification(user_input):
    print('User Verified')
    print('☑️')


print(resp.response)
print(query)

user_input = input("Is this response satisfactory ?")
if user_input.lower() == "yes":
  check_verification(user_input)
else:
  print("Not User Verified")

def compare_strings(str1, str2):

    matches = 0

    for i in range(min(len(str1), len(str2))):

        if str1[i] == str2[i]:

            matches += 1



    percentage = (matches / min(len(str1), len(str2))) * 1000

    return percentage

accuracy = compare_strings(query, resp.response)
print(f"Accuracy: {accuracy}%")

print(resp.response)

"""https://itsjb13.medium.com/building-a-rag-chatbot-using-llamaindex-groq-with-llama3-chainlit-b1709f770f55

https://docs.llamaindex.ai/en/stable/optimizing/production_rag/
"""